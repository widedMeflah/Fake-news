{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25224,"status":"ok","timestamp":1682794525055,"user":{"displayName":"Wided MEFLAH","userId":"14621791054567712020"},"user_tz":-120},"id":"gUCys6jddVC8","outputId":"c41c6743-500d-4526-9f89-e1d64412e678"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"tXK6a-dIiqAj","executionInfo":{"status":"ok","timestamp":1682794526452,"user_tz":-120,"elapsed":1402,"user":{"displayName":"Wided MEFLAH","userId":"14621791054567712020"}}},"outputs":[],"source":["import pandas as pd\n","df_train = pd.read_csv('/content/drive/MyDrive/ML/HAI817_Projet_train.csv')\n","df_test = pd.read_csv('/content/drive/MyDrive/ML/HAI817_Projet_test.csv')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1959,"status":"ok","timestamp":1682794528406,"user":{"displayName":"Wided MEFLAH","userId":"14621791054567712020"},"user_tz":-120},"id":"RF6BiAf21qW6","outputId":"557d530e-da43-4b1b-9f23-0a3412deec70"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}],"source":["# Importation des différentes librairies, classes et fonctions utilespour le notebook\n","\n","#Sickit learn met régulièrement à jour des versions et \n","#indique des futurs warnings. \n","#ces deux lignes permettent de ne pas les afficher.\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)\n","\n","\n","# librairies générales\n","import pandas as pd\n","import re\n","from tabulate import tabulate\n","import time\n","import numpy as np\n","import pickle\n","import string\n","import base64\n","import sys\n","\n","# librairie affichage\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# librairies scikit learn\n","import sklearn\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.base import TransformerMixin\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import train_test_split\n","from sklearn import metrics\n","from sklearn.model_selection import cross_val_score\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","from sklearn.model_selection import KFold\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import accuracy_score\n","\n","\n","# librairies des classifiers utilisés\n","from sklearn.svm import SVC\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.ensemble import RandomForestClassifier\n","\n","# librairies NLTK\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","from nltk.stem import PorterStemmer \n","from nltk.corpus import stopwords\n","from nltk import word_tokenize \n","\n"," \n","nltk.download('wordnet')\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","stop_words = set(stopwords.words('english')) "]},{"cell_type":"code","execution_count":4,"metadata":{"id":"84i15_2X1Fr3","executionInfo":{"status":"ok","timestamp":1682794529257,"user_tz":-120,"elapsed":858,"user":{"displayName":"Wided MEFLAH","userId":"14621791054567712020"}}},"outputs":[],"source":["my_local_drive='/content/drive/MyDrive/ML/Prof/ML_FDS'\n","\n","# Ajout du path pour les librairies, fonctions et données\n","sys.path.append(my_local_drive)\n","# Se positionner sur le répertoire associé\n","#%cd $my_local_drive\n","\n","#%pwd\n","\n","# fonctions utilities (affichage, confusion, etc.)\n","from MyNLPUtilities import *"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1682794529258,"user":{"displayName":"Wided MEFLAH","userId":"14621791054567712020"},"user_tz":-120},"id":"0KqBSYJHdWHk"},"outputs":[],"source":["df_all = pd.concat([df_train,df_test])\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1682794529259,"user":{"displayName":"Wided MEFLAH","userId":"14621791054567712020"},"user_tz":-120},"id":"oJohKa29qXg0","outputId":"bc2bc729-85ce-4cfd-9fe7-375a3c686e8a"},"outputs":[{"output_type":"stream","name":"stdout","text":["public_id      612\n","text             0\n","title           23\n","our rating       0\n","ID            1264\n","dtype: int64\n"]}],"source":["# compter les valeurs manquantes dans chaque colonne\n","num_missing_values = df_all.isna().sum()\n","print(num_missing_values)"]},{"cell_type":"markdown","source":["**1ère tache de classification {VRAI} vs. {FAUX}**"],"metadata":{"id":"tZTJmYpHQzcX"}},{"cell_type":"markdown","source":["Encodage des classes 1ère tache de classification"],"metadata":{"id":"_sJWofqWLQU4"}},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":462,"status":"ok","timestamp":1682794535910,"user":{"displayName":"Wided MEFLAH","userId":"14621791054567712020"},"user_tz":-120},"id":"sAuLr5smqXjE","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e59e36b0-3f08-4595-fb5f-171d6f3123dc"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-7-a826abee1647>:4: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df_all['classe'] = df_all['our rating'].map({'true': 1, 'false': 2})\n"]}],"source":["df=df_all\n","df_all = df_all[~df_all['our rating'].isin(['mixture', 'other'])]\n","\n","df_all['classe'] = df_all['our rating'].map({'true': 1, 'false': 2})"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":402,"status":"ok","timestamp":1682794539138,"user":{"displayName":"Wided MEFLAH","userId":"14621791054567712020"},"user_tz":-120},"id":"nMqjhQt7Z9ZK","outputId":"a2424863-cb99-492b-c9e9-9223b895d881"},"outputs":[{"output_type":"stream","name":"stdout","text":["2    893\n","1    421\n","Name: classe, dtype: int64\n"]}],"source":["print(df_all[\"classe\"].value_counts())"]},{"cell_type":"markdown","source":["Equilibrage des classes"],"metadata":{"id":"YtwJubklLvej"}},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":415,"status":"ok","timestamp":1682794543585,"user":{"displayName":"Wided MEFLAH","userId":"14621791054567712020"},"user_tz":-120},"id":"GtblP4J7kqJv","outputId":"d7a94984-6696-4517-e597-0fccf3f0ea65"},"outputs":[{"output_type":"stream","name":"stdout","text":["Taille du jeu de données équilibré :  (842, 6)\n","2    421\n","1    421\n","Name: classe, dtype: int64\n"]}],"source":["from sklearn.utils import resample\n","import pandas as pd\n","\n","\n","\n","# Séparer les classes majoritaires et minoritaires\n","df_majority = df_all[df_all['classe'] == 2]\n","df_minority = df_all[df_all['classe'] == 1]\n","\n","# Sous-échantillonner la classe majoritaire\n","df_majority_downsampled = resample(df_majority, \n","                                   replace=False,    # Échantillonnage sans remplacement\n","                                   n_samples=len(df_minority), # Nombre d'échantillons égal à la classe minoritaire\n","                                   random_state=42)  # Pour la reproductibilité\n","\n","# Combiner les classes majoritaire et minoritaire\n","df_balanced = pd.concat([df_majority_downsampled, df_minority])\n","\n","# Afficher la taille du jeu de données équilibré\n","print(\"Taille du jeu de données équilibré : \", df_balanced.shape)\n","\n","df_all = df_balanced\n","print(df_all['classe'].value_counts())"]},{"cell_type":"markdown","metadata":{"id":"-jqEchizbBic"},"source":["Définition de la fonction  MyCleanText"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":357,"status":"ok","timestamp":1682794547964,"user":{"displayName":"Wided MEFLAH","userId":"14621791054567712020"},"user_tz":-120},"id":"1l9cYobAbIjV","outputId":"7d22a390-de31-45f4-8d21-7bab93174c1d"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["import re\n","import string\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","from nltk import word_tokenize\n","nltk.download('wordnet')\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","\n","stop_words = set(stopwords.words('english'))\n","\n","def MyCleanText(X,\n"," lowercase=False, # mettre en minuscule\n"," removestopwords=False, # supprimer les stopwords\n"," removedigit=False, # supprimer les nombres\n"," getstemmer=False, # conserver la racine des termes\n"," getlemmatisation=False # lematisation des termes\n"," ):\n","    \n","    sentence = str(X)\n","    \n","    # suppression des caractères spéciaux\n","    sentence = re.sub(r'[^\\w\\s]',' ', sentence)\n","    \n","    # suppression de tous les caractères uniques\n","    sentence = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', sentence)\n","    \n","    # substitution des espaces multiples par un seul espace\n","    sentence = re.sub(r'\\s+', ' ', sentence, flags=re.I)\n","    \n","    # decoupage en mots\n","    tokens = word_tokenize(sentence)\n","    \n","    if lowercase:\n","        tokens = [token.lower() for token in tokens]\n","\n","    # suppression ponctuation\n","    table = str.maketrans('', '', string.punctuation)\n","    words = [token.translate(table) for token in tokens]\n","    \n","    # suppression des tokens non alphabetique ou numerique\n","    words = [word for word in words if word.isalnum()]\n","\n","    # suppression des tokens numerique\n","    if removedigit:\n","        words = [word for word in words if not word.isdigit()]\n","    \n","    # suppression des stopwords\n","    if removestopwords:\n","        words = [word for word in words if not word in stop_words]\n","    \n","    # lemmatisation\n","    if getlemmatisation:\n","        lemmatizer=WordNetLemmatizer()\n","        words = [lemmatizer.lemmatize(word)for word in words]\n","    \n","    # racinisation\n","    if getstemmer:\n","        ps = PorterStemmer()\n","        words=[ps.stem(word) for word in words]\n","\n","    sentence= ' '.join(words)\n","\n","    return sentence\n"]},{"cell_type":"markdown","metadata":{"id":"bfTRENa2dVLG"},"source":["Définition de la fonction TextNormalizer"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"XT2JPRPGdaoM","executionInfo":{"status":"ok","timestamp":1682794553298,"user_tz":-120,"elapsed":335,"user":{"displayName":"Wided MEFLAH","userId":"14621791054567712020"}}},"outputs":[],"source":["from sklearn.base import BaseEstimator, TransformerMixin\n","class TextNormalizer(BaseEstimator, TransformerMixin):\n","    def __init__(self,\n","                 removestopwords=False, # suppression des stopwords\n","                 lowercase=False,# passage en minuscule\n","                 removedigit=False, # supprimer les nombres\n","                 getstemmer=False,# racinisation des termes\n","                 getlemmatisation=False # lemmatisation des termes\n","                ):\n","        self.lowercase=lowercase\n","        self.getstemmer=getstemmer\n","        self.removestopwords=removestopwords\n","        self.getlemmatisation=getlemmatisation\n","        self.removedigit=removedigit\n","    \n","    def transform(self, X, **transform_params):\n","        # Nettoyage du texte\n","        X=X.copy() # pour conserver le fichier d'origine\n","        return [MyCleanText(text, lowercase=self.lowercase,\n","                            getstemmer=self.getstemmer,\n","                            removestopwords=self.removestopwords,\n","                            getlemmatisation=self.getlemmatisation,\n","                            removedigit=self.removedigit) for text in X]\n","    \n","    def fit(self, X, y=None, **fit_params):\n","        return self\n","    \n","    def fit_transform(self, X, y=None, **fit_params):\n","        return self.fit(X).transform(X)\n","    \n","    def get_params(self, deep=True):\n","        return {\n","            'lowercase':self.lowercase,\n","            'getstemmer':self.getstemmer,\n","            'removestopwords':self.removestopwords,\n","            'getlemmatisation':self.getlemmatisation,\n","            'removedigit':self.removedigit\n","        }\n","    \n","    def set_params(self, **parameters):\n","        for parameter, value in parameters.items():\n","            setattr(self, parameter, value)\n","        return self\n"]},{"cell_type":"markdown","source":["Pipeline de classification de texte avec sélection de features discriminantes"],"metadata":{"id":"Nsdrk7GsPwXa"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_selection import SelectKBest, chi2\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.svm import SVC\n","from sklearn.pipeline import Pipeline\n","\n","\n","\n","# Séparation des données en ensembles d'entraînement et de test\n","X = df_all['text']\n","y = df_all['classe']\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","# Initialisation du pipeline\n","pipe = Pipeline([\n","    (\"cleaner\", TextNormalizer(removestopwords=True, lowercase=True, removedigit=True, getstemmer=False, getlemmatisation=False)),\n","    (\"TfidfVectorizer\", TfidfVectorizer()), \n","    (\"selector\", SelectKBest(chi2, k=int(len(X_train)*0.1))),\n","\n","    (\"svm\", SVC(C=1, gamma=0.001, kernel='linear'))\n","])\n","\n","\n","\n","\n","# Ajustement du pipeline sur les données d'entraînement\n","pipe.fit(X_train, y_train)\n","\n","# Utilisation de SelectKBest pour sélectionner les 10% des features les plus discriminantes\n","\n","selector= SelectKBest(chi2, k=int(len(X_train)*0.1))\n","\n","\n","\n","\n","# Récupération de la liste des features discriminantes\n","feature_names = list(pipe.named_steps['TfidfVectorizer'].vocabulary_.keys())\n","mask = pipe.named_steps['selector'].get_support()\n","selected_features = [feature_names[i] for i, val in enumerate(mask) if val]\n","\n","# Affichage des features sélectionnées\n","print(\"Liste des features discriminantes :\")\n","for feature in selected_features:\n","    print(feature)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F-270ys0BuoR","executionInfo":{"status":"ok","timestamp":1682794559797,"user_tz":-120,"elapsed":2359,"user":{"displayName":"Wided MEFLAH","userId":"14621791054567712020"}},"outputId":"e75183b3-db77-4c5c-c74d-d2d57872251c"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Liste des features discriminantes :\n","inspectors\n","hipster\n","wedding\n","captain\n","specifically\n","miami\n","alleged\n","politico\n","failed\n","disapproved\n","js\n","consumed\n","asteroidwatch\n","difficulties\n","unable\n","redeem\n","identification\n","code\n","withdrew\n","owning\n","accepted\n","wigdortz\n","floating\n","transantarctic\n","generates\n","anywhere\n","vdawtd\n","resignation\n","shuttered\n","meant\n","tossed\n","introducing\n","township\n","kansas\n","israelis\n","dealers\n","rosemary\n","assesses\n","leo\n","unbound\n","coaches\n","embargoes\n","roberta\n","cmvdh9rpgl\n","pi\n","nazis\n","reliefs\n","limousine\n","weighs\n","volatile\n","neckline\n","pasteur\n","lawmaker\n","mystic\n","con\n","shippers\n","alok\n","que\n","yenne\n","drumming\n","alt\n","widest\n","overused\n","montagnier\n","cajeput\n","emblematic\n","dropout\n"]}]},{"cell_type":"markdown","source":["**2ème tache de classification {VRAI ou FAUX} vs. {AUTRE}**"],"metadata":{"id":"SyLvIP1lREN1"}},{"cell_type":"code","source":["import pandas as pd\n","\n","\n","df2=df\n","# Encoder les classes en deux catégories : \"VRAI ou FAUX\" et \"AUTRE\"\n","df['classe'] = df['our rating'].replace(['mixture', 'other'], 'AUTRE')\n","df['classe'] = df['classe'].replace(['true', 'false'], 'TrueFalse')\n","\n","\n","df['classe'] = df['classe'].map({'TrueFalse': 1, 'AUTRE': 2})\n","\n","from sklearn.utils import resample\n","import pandas as pd\n","\n","\n","\n","# Séparer les classes majoritaires et minoritaires\n","df_majority = df[df['classe'] == 1]\n","df_minority = df[df['classe'] == 2]\n","\n","# Sous-échantillonner la classe majoritaire\n","df_majority_downsampled = resample(df_majority, \n","                                   replace=False,    # Échantillonnage sans remplacement\n","                                   n_samples=len(df_minority), # Nombre d'échantillons égal à la classe minoritaire\n","                                   random_state=42)  # Pour la reproductibilité\n","\n","# Combiner les classes majoritaire et minoritaire\n","df_balanced = pd.concat([df_majority_downsampled, df_minority])\n","\n","\n","\n","df = df_balanced\n","print(df['classe'].value_counts())\n","\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_selection import SelectKBest, chi2\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.svm import SVC\n","from sklearn.pipeline import Pipeline\n","\n","\n","\n","# Séparation des données en ensembles d'entraînement et de test\n","X = df['text']\n","y = df['classe']\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","# Initialisation du pipeline\n","pipe = Pipeline([\n","    (\"cleaner\", TextNormalizer(removestopwords=True, lowercase=True, removedigit=True, getstemmer=False, getlemmatisation=False)),\n","    (\"TfidfVectorizer\", TfidfVectorizer()), \n","    (\"selector\", SelectKBest(chi2, k=int(len(X_train)*0.1))),\n","\n","    (\"svm\", SVC(C=1, gamma=0.001, kernel='linear'))\n","])\n","\n","\n","\n","\n","# Ajustement du pipeline sur les données d'entraînement\n","pipe.fit(X_train, y_train)\n","\n","# Utilisation de SelectKBest pour sélectionner les 10% des features les plus discriminantes\n","\n","selector= SelectKBest(chi2, k=int(len(X_train)*0.1))\n","\n","\n","\n","\n","# Récupération de la liste des features discriminantes\n","feature_names = list(pipe.named_steps['TfidfVectorizer'].vocabulary_.keys())\n","mask = pipe.named_steps['selector'].get_support()\n","selected_features = [feature_names[i] for i, val in enumerate(mask) if val]\n","\n","# Affichage des features sélectionnées\n","print(\"Liste des features discriminantes :\")\n","for feature in selected_features:\n","    print(feature)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KckJUElCSF9y","executionInfo":{"status":"ok","timestamp":1682794692830,"user_tz":-120,"elapsed":2764,"user":{"displayName":"Wided MEFLAH","userId":"14621791054567712020"}},"outputId":"7994d0a6-f112-4c86-baa7-b65fc7d81726"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["1    562\n","2    562\n","Name: classe, dtype: int64\n","Liste des features discriminantes :\n","related\n","goal\n","officials\n","river\n","live\n","contaminated\n","genuine\n","danish\n","bully\n","proponents\n","socioeconomic\n","respectively\n","broke\n","capped\n","officially\n","coronaviruses\n","cybercriminals\n","cybercrime\n","toxic\n","homeless\n","structures\n","deployed\n","strategies\n","tropics\n","deceased\n","meetings\n","dominic\n","tighten\n","pains\n","marshallese\n","battling\n","preexisting\n","incarnate\n","unfathomably\n","tnt\n","pml\n","individualism\n","harwood\n","donates\n","zoologist\n","transcribes\n","attract\n","carswell\n","pandemics\n","hodeida\n","vanishingly\n","treading\n","enablers\n","valter\n","coworkers\n","salvadoran\n","javier\n","payroll\n","positively\n","rulers\n","kensington\n","apprehending\n","rushing\n","pitch\n","reims\n","aahs\n","shanxi\n","biomedical\n","repelled\n","firefighter\n","interfered\n","faire\n","demoralized\n","odegbune\n","sleek\n","quantitative\n","mercedes\n","fermi\n","compresses\n","dayside\n","riddle\n","merging\n","wilshaw\n","julian\n","25nmol\n","unloaded\n","roaring\n","rebadging\n","5e\n","consol\n","sapphire\n","vogue\n","sunglasses\n","shortcuts\n"]}]},{"cell_type":"markdown","source":["**3ème tache de classification {VRAI} vs. {FAUX} vs. {MIXTE} vs. {AUTRE}**"],"metadata":{"id":"DeNj26CBRQWU"}},{"cell_type":"code","source":["\n","\n","df2['classe'] = df2['our rating'].map({'true': 1, 'false': 2, 'mixture': 3, 'other': 4})\n","\n","\n","# Downsampling de la classe majoritaire\n","false_downsampled = resample(df2[df2['our rating'] == 'false'], replace=False, n_samples=250, random_state=42)\n","\n","# Upsampling de la classe minoritaire\n","true_upsampled = resample(df2[df2['our rating'] == 'true'], replace=False, n_samples=250, random_state=42)\n","mixture_upsampled = resample(df2[df2['our rating'] == 'mixture'], replace=False, n_samples=250, random_state=42)\n","other_upsampled = resample(df2[df2['our rating'] == 'other'], replace=True, n_samples=250, random_state=42)\n","\n","# Concaténer les données échantillonnées\n","balanced_data = pd.concat([false_downsampled, true_upsampled, mixture_upsampled, other_upsampled])\n","\n","# Afficher les nouvelles proportions des classes\n","df2 = balanced_data\n","\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_selection import SelectKBest, chi2\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.svm import SVC\n","from sklearn.pipeline import Pipeline\n","\n","\n","\n","# Séparation des données en ensembles d'entraînement et de test\n","X = df2['text']\n","y = df2['classe']\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","# Initialisation du pipeline\n","pipe = Pipeline([\n","    (\"cleaner\", TextNormalizer(removestopwords=True, lowercase=True, removedigit=True, getstemmer=False, getlemmatisation=False)),\n","    (\"TfidfVectorizer\", TfidfVectorizer()), \n","    (\"selector\", SelectKBest(chi2, k=int(len(X_train)*0.1))),\n","\n","    (\"svm\", SVC(C=1, gamma=0.001, kernel='linear'))\n","])\n","\n","\n","\n","\n","# Ajustement du pipeline sur les données d'entraînement\n","pipe.fit(X_train, y_train)\n","\n","# Utilisation de SelectKBest pour sélectionner les 10% des features les plus discriminantes\n","\n","selector= SelectKBest(chi2, k=int(len(X_train)*0.1))\n","\n","\n","\n","\n","# Récupération de la liste des features discriminantes\n","feature_names = list(pipe.named_steps['TfidfVectorizer'].vocabulary_.keys())\n","mask = pipe.named_steps['selector'].get_support()\n","selected_features = [feature_names[i] for i, val in enumerate(mask) if val]\n","\n","# Affichage des features sélectionnées\n","print(\"Liste des features discriminantes :\")\n","for feature in selected_features:\n","    print(feature)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gz8JFb0CTaIg","executionInfo":{"status":"ok","timestamp":1682794811119,"user_tz":-120,"elapsed":2633,"user":{"displayName":"Wided MEFLAH","userId":"14621791054567712020"}},"outputId":"87ea66c6-028a-4018-9978-42480bda2137"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Liste des features discriminantes :\n","negotiating\n","society\n","flgov\n","debris\n","removing\n","county\n","dominion\n","photo\n","pregnancies\n","win\n","intended\n","misleadingly\n","planning\n","smaller\n","newspaper\n","terminated\n","john\n","injuries\n","imposing\n","prices\n","mohammed\n","tour\n","partly\n","icebridge\n","grindhouse\n","interactive\n","sturgeon\n","salinity\n","rahmstorf\n","csl\n","subtleties\n","orwellian\n","flourish\n","listened\n","notable\n","zimbabwe\n","object\n","bonn\n","showcasing\n","flynn\n","yelled\n","forbidding\n","fainted\n","korean\n","jakobshavn\n","conditioner\n","lifesitenews\n","ovulation\n","messaged\n","bandana\n","convenience\n","palliative\n","injure\n","negotiation\n","ahold\n","sicker\n","daddy\n","cancun\n","shah\n","physiology\n","streaming\n","hutchinson\n","golfers\n","nanoscience\n","gill\n","psn\n","contended\n","attain\n","sceptic\n","mao\n","arlene\n","mirna\n","snaking\n","dad\n","ealing\n","liveability\n","robbed\n","andymurray\n","ponzi\n","millimeters\n"]}]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}